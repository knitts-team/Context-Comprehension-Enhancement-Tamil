{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RootSeqEnglish.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8Nb5zC0por6b"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### OM NAMO NARAYANA"
      ],
      "metadata": {
        "id": "BKh_8LTp7LZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation of dataset\n",
        "**Output:** A Pytorch Dataset class returning sequence of root words in English <br/>\n",
        "**Input:** Preprocessed English sentences <br/>\n",
        "- Dataset: Wikipedia dataset <br/>\n",
        "- Lemmatizer: Spacy <br/>\n",
        "- Model: T5 <br/>\n",
        "- Pretrained on: C4 Dataset <br/>\n",
        "- Loss: ?\n",
        "\n"
      ],
      "metadata": {
        "id": "Chh4rl4w7OMH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0evkkxyt7ItY"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UvNaRvDyvyXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project='root2seq')"
      ],
      "metadata": {
        "id": "RqUEbAcEuXGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/drive/My Drive/'\n",
        "checkpoint_dir = root_dir + 'checkpoints/'"
      ],
      "metadata": {
        "id": "hW-bceAYv-Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import spacy\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader \n",
        "\n",
        "class T5Dataset(Dataset):\n",
        "    \"\"\"T5 root2seq dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, tokenizer, transform=lambda k:k):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset (dataset): Dataloader from datasets.\n",
        "            tokenizer (Tokenizer): To tokenizer input and output sentence.\n",
        "            transform (function): Any transformation function\n",
        "        \"\"\"\n",
        "        self.dataset=dataset\n",
        "        self.nlp = spacy.load('en')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.dataset[idx][\"sentence\"]\n",
        "        doc = self.nlp(sentence)\n",
        "        \n",
        "        li=[]\n",
        "        for token in doc:\n",
        "            li.append(token.lemma_)\n",
        "\n",
        "        input_sentence = ''\n",
        "        for input_word in li: input_sentence += input_word + ' ' \n",
        "\n",
        "\n",
        "        input_tokens = tokenizer(self.transform(input_sentence), max_length=1024, return_tensors=\"pt\")\n",
        "        output_tokens = tokenizer(self.transform(sentence), max_length=1024, return_tensors=\"pt\")\n",
        "        # input_tokens = tokenizer(input_sentence, max_length=128, padding='max_length', return_tensors=\"pt\")\n",
        "        # output_tokens = tokenizer(sentence, max_length=128, padding='max_length', return_tensors=\"pt\")\n",
        "\n",
        "        # print('input_tokens.shape: ', input_tokens.input_ids.shape, 'output_tokens.shape: ', output_tokens.input_ids.shape)\n",
        "\n",
        "        return {\"input_tokens\": input_tokens, \"output_tokens\": output_tokens}"
      ],
      "metadata": {
        "id": "enuYOIYz_cLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import get_scheduler\n",
        "import os\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "\n",
        "wandb.config.update({\n",
        "    'epochs': 10,\n",
        "    'tokenizer': 't5-small',\n",
        "    'optimizer': 'nn.NLLLoss',\n",
        "})\n",
        "\n",
        "load_model = True\n",
        "config = wandb.config\n",
        "config.epochs = 10\n",
        "config.TOKENIZER = AutoTokenizer.from_pretrained(\"t5-small\") \n",
        "\n",
        "def train(model, tokenizer, model_optimizer, criterion, dataloader, epochs = 10, debug=False, checkpoint_dir = \"/\", model_name = \"unknown\", **kwargs):\n",
        "\n",
        "\n",
        "  num_training_steps = epochs * len(dataloader)\n",
        "\n",
        "  lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "\n",
        "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  print_freq = 200\n",
        "  save_freq = 200\n",
        "  model_optimizer.zero_grad()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    t = 0\n",
        "    if(epoch % 2 == 0): \n",
        "      if(not os.path.isdir(checkpoint_dir + model_name + '/')):\n",
        "        os.makedirs(checkpoint_dir + model_name + '/')\n",
        "\n",
        "      torch.save(model.state_dict(), checkpoint_dir + model_name + '/' +datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")+'.pt')\n",
        "      print('checkpoint saved')\n",
        "    for data in tqdm(dataloader):\n",
        "      t = t + 1\n",
        "      input_ids = data['input_tokens']['input_ids'].squeeze(1)\n",
        "      attention_mask = data['input_tokens']['attention_mask'].squeeze(1)\n",
        "      output = data['output_tokens']['input_ids'].squeeze(1)\n",
        "      input_ids= input_ids.to(device)\n",
        "      output = output.to(device)\n",
        "      attention_mask = attention_mask.to(device)\n",
        "      if(debug): print('input_ids.shape: ', input_ids.shape)\n",
        "      if(debug): print('device:', device, ' input_ids.device: ', input_ids.device, ' output.device: ', output.device, ' attention_mask.device: ', attention_mask.device)\n",
        "\n",
        "      if(debug):predicted_tokens = model(input_ids = input_ids, attention_mask = attention_mask,labels = output)\n",
        "      else: loss = model(input_ids = input_ids, labels = output).loss\n",
        "      if(debug): print('predicted_tokens: ', predicted_tokens, 'logits.shape: ', predicted_tokens['logits'].shape) # logits.shape [1, x, 32128]\n",
        "\n",
        "      if(not debug): wandb.log({'loss': loss.item()})\n",
        "\n",
        "      if(t % print_freq == 1):\n",
        "        print('t:', t, 'loss: ', loss.item(), '\\ntarget_sentence:', tokenizer.decode(output[0]))\n",
        "        predicted_sentence = model.generate(torch.unsqueeze(input_ids[0], 0), num_beams=4, max_length=10,)\n",
        "        print('input_sentence: ', tokenizer.decode(input_ids[0], skip_special_tokens=True), '\\npredicted_sentence: ', tokenizer.decode(predicted_sentence[0], skip_special_tokens=True))\n",
        "\n",
        "      # epoch_loss += predicted_tokens['loss']\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      model_optimizer.step()\n",
        "      lr_scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "# model_name = \"csebuetnlp/mT5_multilingual_XLSum\"      \n",
        "model_name = \"t5-small\"      \n",
        "\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_transform = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
        "tokenizer_transform = lambda k: k\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.NLLLoss()\n",
        "dataset = load_dataset('glue', 'cola', split='train')\n",
        "t5dataset = T5Dataset(dataset, tokenizer, transform = tokenizer_transform)\n",
        "# dataloader = torch.utils.data.DataLoader(t5dataset, batch_size = 4, shuffle=True)\n",
        "if(load_model):\n",
        "  try:\n",
        "    ls = os.listdir(checkpoint_dir + model_name + '/')\n",
        "    ls.sort()\n",
        "    latest_file = checkpoint_dir + model_name + '/' + ls[-1]\n",
        "    model.load_state_dict(torch.load(latest_file))\n",
        "    print(\"loaded latest checkpoint\")\n",
        "  except:\n",
        "    print(\"can't load model\")\n",
        "train(model, tokenizer, optimizer, criterion, t5dataset, 10, checkpoint_dir = checkpoint_dir, model_name=model_name)"
      ],
      "metadata": {
        "id": "9cpAOoYDD8mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IhgkAwuutTVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Archeive"
      ],
      "metadata": {
        "id": "8Nb5zC0por6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "w3m2RHm2_yYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARTICLE_TO_SUMMARIZE = \"summarize: My friends are cool but they eat too many carbs.\"\n",
        "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "# Generate Summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=6,)\n",
        "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))"
      ],
      "metadata": {
        "id": "WhClAmxoAATg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # training\n",
        "# input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
        "# labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
        "# outputs = model(input_ids=input_ids, labels=labels)\n",
        "# loss = outputs.loss\n",
        "# logits = outputs.logits\n",
        "\n",
        "# inference\n",
        "input_ids = tokenizer(\n",
        "    \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
        ").input_ids  # Batch size 1\n",
        "outputs = model.generate(input_ids, num_beams=4, max_length=5,)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "# studies have shown that owning a dog is good for you."
      ],
      "metadata": {
        "id": "sBH17zOLCfBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# teacher_forcing_ratio = 0.5\n",
        "\n",
        "# def T5seq(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
        "#    model_optimizer.zero_grad()\n",
        "\n",
        "#    input_length = input_tensor.size(0)\n",
        "#    loss = 0\n",
        "#    epoch_loss = 0\n",
        "#    # print(input_tensor.shape)\n",
        "\n",
        "#    output = model(input_tensor, target_tensor)\n",
        "\n",
        "#    num_iter = output.size(0)\n",
        "#    print(num_iter)\n",
        "\n",
        "# #calculate the loss from a predicted sentence with the expected result\n",
        "#    for ot in range(num_iter):\n",
        "#        loss += criterion(output[ot], target_tensor[ot])\n",
        "\n",
        "#    loss.backward()\n",
        "#    model_optimizer.step()\n",
        "#    epoch_loss = loss.item() / num_iter\n",
        "\n",
        "#    return epoch_loss\n",
        "\n",
        "# def trainModel(model, source, target, pairs, num_iteration=20000):\n",
        "#    model.train()\n",
        "\n",
        "#    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "#    criterion = nn.NLLLoss()\n",
        "#    total_loss_iterations = 0\n",
        "\n",
        "#    training_pairs = [tensorsFromPair(source, target, random.choice(pairs))\n",
        "#                      for i in range(num_iteration)]\n",
        "  \n",
        "#    for iter in range(1, num_iteration+1):\n",
        "#        training_pair = training_pairs[iter - 1]\n",
        "#        input_tensor = training_pair[0]\n",
        "#        target_tensor = training_pair[1]\n",
        "\n",
        "#        loss = clacModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
        "\n",
        "#        total_loss_iterations += loss\n",
        "\n",
        "#        if iter % 5000 == 0:\n",
        "#            avarage_loss= total_loss_iterations / 5000\n",
        "#            total_loss_iterations = 0\n",
        "#            print('%d %.4f' % (iter, avarage_loss))\n",
        "          \n",
        "#    torch.save(model.state_dict(), 'mytraining.pt')\n",
        "#    return model"
      ],
      "metadata": {
        "id": "LTOdAhvZDs8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ClCEt5rpC_NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "# import pandas as pd\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "\n",
        "# df = pd.read_csv(\"\")\n",
        "\n",
        "dataset = load_dataset('glue', 'cola', split='train')\n",
        "print(dataset[0])\n",
        "# dataset[0].pop(\"label\")\n",
        "# dataset[0].pop(\"idx\")\n",
        "text = [dataset[0][\"sentence\"]]\n",
        "print(text)\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "# text=(\"Our friends won't buy this analysis, let alone the next one we propose.\")\n",
        "print(text)\n",
        "pass\n",
        "doc = [nlp(t) for t in text]\n",
        "print(doc)\n",
        "\n",
        "total_li=[]\n",
        "li=[]\n",
        "for lines in doc:\n",
        "  li=[]\n",
        "  for token in lines:\n",
        "    li.append(token.lemma_)\n",
        "  total_li.append(li)\n",
        "print(total_li)"
      ],
      "metadata": {
        "id": "lJNmNN_L_zhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_list = ['-PRON-', 'friend', 'will', 'not', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', '-PRON-', 'propose', '.']\n",
        "input_sentence = ''\n",
        "for input_word in input_list: input_sentence += input_word + ' '\n",
        "input_sentence = input_sentence.strip()\n",
        "tokenizer(input_sentence , return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "VrV13Fy6_3yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "dataset = load_dataset('glue', 'cola', split='train')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "dataset = T5Dataset(dataset, tokenizer)\n",
        "print('dataset[0]: ', dataset[0])\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "# print(next(iter(dataloader)))"
      ],
      "metadata": {
        "id": "WBxsKu2B_xr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import os\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "ls = os.listdir(checkpoint_dir)\n",
        "ls.sort()\n",
        "print('ls: ', ls)\n",
        "latest_file = checkpoint_dir + ls[-1]\n",
        "model.load_state_dict(torch.load(latest_file))\n",
        "print(\"loaded latest file\")\n",
        "print(\"can't load model\")"
      ],
      "metadata": {
        "id": "nLfvhof6C-Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "x = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "print(x)"
      ],
      "metadata": {
        "id": "IfghO8-twfLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "example_english_phrase = \"UN Chief Says There Is No Military Solution in Syria\"\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "inputs = tokenizer(example_english_phrase, return_tensors=\"pt\")\n",
        "\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=5)\n",
        "x = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0]\n",
        "print(x)"
      ],
      "metadata": {
        "id": "5akkgAvvgwwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
        "\n",
        "article_text = \"\"\"Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.\"\"\"\n",
        "\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "input_ids = tokenizer(\n",
        "    [WHITESPACE_HANDLER(article_text)],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ")[\"input_ids\"]\n",
        "\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=84,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4\n",
        ")[0]\n",
        "\n",
        "summary = tokenizer.decode(\n",
        "    output_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "sGT777VegHRZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}