{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKh_8LTp7LZr"
   },
   "source": [
    "### OM NAMO NARAYANA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chh4rl4w7OMH"
   },
   "source": [
    "### Preparation of dataset\n",
    "**Output:** A Pytorch Dataset class returning sequence of root words in English <br/>\n",
    "**Input:** Preprocessed English sentences <br/>\n",
    "- Dataset: Wikipedia dataset <br/>\n",
    "- Lemmatizer: Spacy <br/>\n",
    "- Model: T5 <br/>\n",
    "- Pretrained on: C4 Dataset <br/>\n",
    "- Loss: ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0evkkxyt7ItY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (4.11.3)\n",
      "Requirement already satisfied: dill in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (4.63.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (4.17.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: six in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (0.1.96)\n",
      "Requirement already satisfied: wandb in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (0.12.11)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: setproctitle in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (1.2.2)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: pathtools in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: yaspin>=1.0.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (2.1.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (1.5.8)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: PyYAML in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.11.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.8)\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvNaRvDyvyXf"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RqUEbAcEuXGC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mteam-knitts\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/ubuntu/miniconda3/envs/fyp-1/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/Context-Comprehension-Enhancement-Tamil/notebooks/wandb/run-20220323_184749-3fkj7ku7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/team-knitts/root2seq/runs/3fkj7ku7\" target=\"_blank\">radiant-haze-15</a></strong> to <a href=\"https://wandb.ai/team-knitts/root2seq\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/team-knitts/root2seq/runs/3fkj7ku7?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f05ac097150>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='root2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hW-bceAYv-Vj"
   },
   "outputs": [],
   "source": [
    "# root_dir = '/content/drive/My Drive/'\n",
    "root_dir = '/home/ubuntu/Context-Comprehension-Enhancement-Tamil/'\n",
    "checkpoint_dir = root_dir + 'checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "enuYOIYz_cLG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "class T5Dataset(Dataset):\n",
    "    \"\"\"T5 root2seq dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer, transform=lambda k:k):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (dataset): Dataloader from datasets.\n",
    "            tokenizer (Tokenizer): To tokenizer input and output sentence.\n",
    "            transform (function): Any transformation function\n",
    "        \"\"\"\n",
    "        self.dataset=dataset\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.dataset[idx][\"sentence\"]\n",
    "        doc = self.nlp(sentence)\n",
    "        \n",
    "        li=[]\n",
    "        for token in doc:\n",
    "            li.append(token.lemma_)\n",
    "\n",
    "        input_sentence = ''\n",
    "        for input_word in li: input_sentence += input_word + ' ' \n",
    "\n",
    "\n",
    "        # input_tokens = tokenizer(self.transform(input_sentence), max_length=1024, return_tensors=\"pt\")\n",
    "        # output_tokens = tokenizer(self.transform(sentence), max_length=1024, return_tensors=\"pt\")\n",
    "        input_tokens = tokenizer(self.transform(input_sentence), max_length=256, padding='max_length', return_tensors=\"pt\")\n",
    "        output_tokens = tokenizer(self.transform(sentence), max_length=256, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "        # print('input_tokens.shape: ', input_tokens.input_ids.shape, 'output_tokens.shape: ', output_tokens.input_ids.shape)\n",
    "\n",
    "        return {\"input_tokens\": input_tokens, \"output_tokens\": output_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cpAOoYDD8mY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded latest checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                      | 0/8551 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 1 loss:  1.247257947921753 \n",
      "target_sentence: Our friends won't buy this analysis, let alone the next one we propose.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                              | 1/8551 [00:00<34:25,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sentence:  our friend will not buy this analysis, let alone the next one we propose. \n",
      "predicted_sentence:  Unser Freund wird diese Analyse nicht\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Š                                           | 152/8551 [00:11<10:25, 13.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import get_scheduler\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "wandb.config.update({\n",
    "    'epochs': 10,\n",
    "    'tokenizer': 't5-small',\n",
    "    'optimizer': 'nn.NLLLoss',\n",
    "})\n",
    "\n",
    "load_model = True\n",
    "config = wandb.config\n",
    "config.epochs = 10\n",
    "config.TOKENIZER = AutoTokenizer.from_pretrained(\"t5-small\") \n",
    "\n",
    "def train(model, tokenizer, model_optimizer, criterion, dataloader, epochs = 10, debug=False, checkpoint_dir = \"/\", model_name = \"unknown\", **kwargs):\n",
    "\n",
    "\n",
    "  num_training_steps = epochs * len(dataloader)\n",
    "\n",
    "  lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "\n",
    "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "  model.to(device)\n",
    "\n",
    "  print_freq = 200\n",
    "  save_freq = 200\n",
    "  model_optimizer.zero_grad()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    t = 0\n",
    "    if(epoch % 2 == 1): \n",
    "      if(not os.path.isdir(checkpoint_dir + model_name + '/')):\n",
    "        os.makedirs(checkpoint_dir + model_name + '/')\n",
    "\n",
    "      torch.save(model.state_dict(), checkpoint_dir + model_name + '/' +datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")+'.pt')\n",
    "      print('checkpoint saved')\n",
    "    for data in tqdm(dataloader):\n",
    "      t = t + 1\n",
    "      input_ids = data['input_tokens']['input_ids'].squeeze(1)\n",
    "      attention_mask = data['input_tokens']['attention_mask'].squeeze(1)\n",
    "      output = data['output_tokens']['input_ids'].squeeze(1)\n",
    "      input_ids= input_ids.to(device)\n",
    "      output = output.to(device)\n",
    "      attention_mask = attention_mask.to(device)\n",
    "      if(debug): print('input_ids.shape: ', input_ids.shape)\n",
    "      if(debug): print('device:', device, ' input_ids.device: ', input_ids.device, ' output.device: ', output.device, ' attention_mask.device: ', attention_mask.device)\n",
    "\n",
    "      if(debug):predicted_tokens = model(input_ids = input_ids, attention_mask = attention_mask,labels = output)\n",
    "      else: loss = model(input_ids = input_ids, labels = output).loss\n",
    "      if(debug): print('predicted_tokens: ', predicted_tokens, 'logits.shape: ', predicted_tokens['logits'].shape) # logits.shape [1, x, 32128]\n",
    "\n",
    "      if(not debug): wandb.log({'loss': loss.item()})\n",
    "\n",
    "      if(t % print_freq == 1):\n",
    "        print('t:', t, 'loss: ', loss.item(), '\\ntarget_sentence:', tokenizer.decode(output[0]))\n",
    "        predicted_sentence = model.generate(torch.unsqueeze(input_ids[0], 0), num_beams=4, max_length=10,)\n",
    "        print('input_sentence: ', tokenizer.decode(input_ids[0], skip_special_tokens=True), '\\npredicted_sentence: ', tokenizer.decode(predicted_sentence[0], skip_special_tokens=True))\n",
    "\n",
    "      # epoch_loss += predicted_tokens['loss']\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      model_optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "# model_name = \"csebuetnlp/mT5_multilingual_XLSum\"      \n",
    "model_name = \"t5-small\"      \n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_transform = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "tokenizer_transform = lambda k: k\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss()\n",
    "dataset = load_dataset('glue', 'cola', split='train')\n",
    "# t5dataset = T5Dataset(dataset, tokenizer, transform = tokenizer_transform)\n",
    "dataloader = torch.utils.data.DataLoader(t5dataset, batch_size = 4, shuffle=True)\n",
    "if(load_model):\n",
    "  try:\n",
    "    ls = os.listdir(checkpoint_dir + model_name + '/')\n",
    "    ls.sort()\n",
    "    latest_file = checkpoint_dir + model_name + '/' + ls[-1]\n",
    "    model.load_state_dict(torch.load(latest_file))\n",
    "    print(\"loaded latest checkpoint\")\n",
    "  except:\n",
    "    print(\"can't load model\")\n",
    "train(model, tokenizer, optimizer, criterion, t5dataset, 10, checkpoint_dir = checkpoint_dir, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhgkAwuutTVo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Nb5zC0por6b",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Archeive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3m2RHm2_yYU"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhClAmxoAATg"
   },
   "outputs": [],
   "source": [
    "ARTICLE_TO_SUMMARIZE = \"summarize: My friends are cool but they eat too many carbs.\"\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=6,)\n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBH17zOLCfBI"
   },
   "outputs": [],
   "source": [
    "# # training\n",
    "# input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "# labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "# outputs = model(input_ids=input_ids, labels=labels)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits\n",
    "\n",
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids, num_beams=4, max_length=5,)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# studies have shown that owning a dog is good for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTOdAhvZDs8s"
   },
   "outputs": [],
   "source": [
    "# teacher_forcing_ratio = 0.5\n",
    "\n",
    "# def T5seq(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
    "#    model_optimizer.zero_grad()\n",
    "\n",
    "#    input_length = input_tensor.size(0)\n",
    "#    loss = 0\n",
    "#    epoch_loss = 0\n",
    "#    # print(input_tensor.shape)\n",
    "\n",
    "#    output = model(input_tensor, target_tensor)\n",
    "\n",
    "#    num_iter = output.size(0)\n",
    "#    print(num_iter)\n",
    "\n",
    "# #calculate the loss from a predicted sentence with the expected result\n",
    "#    for ot in range(num_iter):\n",
    "#        loss += criterion(output[ot], target_tensor[ot])\n",
    "\n",
    "#    loss.backward()\n",
    "#    model_optimizer.step()\n",
    "#    epoch_loss = loss.item() / num_iter\n",
    "\n",
    "#    return epoch_loss\n",
    "\n",
    "# def trainModel(model, source, target, pairs, num_iteration=20000):\n",
    "#    model.train()\n",
    "\n",
    "#    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "#    criterion = nn.NLLLoss()\n",
    "#    total_loss_iterations = 0\n",
    "\n",
    "#    training_pairs = [tensorsFromPair(source, target, random.choice(pairs))\n",
    "#                      for i in range(num_iteration)]\n",
    "  \n",
    "#    for iter in range(1, num_iteration+1):\n",
    "#        training_pair = training_pairs[iter - 1]\n",
    "#        input_tensor = training_pair[0]\n",
    "#        target_tensor = training_pair[1]\n",
    "\n",
    "#        loss = clacModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
    "\n",
    "#        total_loss_iterations += loss\n",
    "\n",
    "#        if iter % 5000 == 0:\n",
    "#            avarage_loss= total_loss_iterations / 5000\n",
    "#            total_loss_iterations = 0\n",
    "#            print('%d %.4f' % (iter, avarage_loss))\n",
    "          \n",
    "#    torch.save(model.state_dict(), 'mytraining.pt')\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClCEt5rpC_NT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJNmNN_L_zhE"
   },
   "outputs": [],
   "source": [
    " \n",
    "# import pandas as pd\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "\n",
    "# df = pd.read_csv(\"\")\n",
    "\n",
    "dataset = load_dataset('glue', 'cola', split='train')\n",
    "print(dataset[0])\n",
    "# dataset[0].pop(\"label\")\n",
    "# dataset[0].pop(\"idx\")\n",
    "text = [dataset[0][\"sentence\"]]\n",
    "print(text)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "# text=(\"Our friends won't buy this analysis, let alone the next one we propose.\")\n",
    "print(text)\n",
    "pass\n",
    "doc = [nlp(t) for t in text]\n",
    "print(doc)\n",
    "\n",
    "total_li=[]\n",
    "li=[]\n",
    "for lines in doc:\n",
    "  li=[]\n",
    "  for token in lines:\n",
    "    li.append(token.lemma_)\n",
    "  total_li.append(li)\n",
    "print(total_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrV13Fy6_3yY"
   },
   "outputs": [],
   "source": [
    "input_list = ['-PRON-', 'friend', 'will', 'not', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', '-PRON-', 'propose', '.']\n",
    "input_sentence = ''\n",
    "for input_word in input_list: input_sentence += input_word + ' '\n",
    "input_sentence = input_sentence.strip()\n",
    "tokenizer(input_sentence , return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBxsKu2B_xr0"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "dataset = load_dataset('glue', 'cola', split='train')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "dataset = T5Dataset(dataset, tokenizer)\n",
    "print('dataset[0]: ', dataset[0])\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "# print(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLfvhof6C-Pq"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "ls = os.listdir(checkpoint_dir)\n",
    "ls.sort()\n",
    "print('ls: ', ls)\n",
    "latest_file = checkpoint_dir + ls[-1]\n",
    "model.load_state_dict(torch.load(latest_file))\n",
    "print(\"loaded latest file\")\n",
    "print(\"can't load model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfghO8-twfLK"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "x = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5akkgAvvgwwv"
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "example_english_phrase = \"UN Chief Says There Is No Military Solution in Syria\"\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "inputs = tokenizer(example_english_phrase, return_tensors=\"pt\")\n",
    "\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=5)\n",
    "x = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGT777VegHRZ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "\n",
    "article_text = \"\"\"Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.\"\"\"\n",
    "\n",
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    [WHITESPACE_HANDLER(article_text)],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")[\"input_ids\"]\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=84,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4\n",
    ")[0]\n",
    "\n",
    "summary = tokenizer.decode(\n",
    "    output_ids,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8Nb5zC0por6b"
   ],
   "name": "RootSeqEnglish.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
